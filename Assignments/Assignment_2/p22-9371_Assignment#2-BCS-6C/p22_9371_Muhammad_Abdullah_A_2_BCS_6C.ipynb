{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 5,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cFgO-_ocSWpb",
        "outputId": "5fd2165d-d03e-4502-df98-27f17afb8b3c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Attempting to load: data0.txt\n",
            "Loaded 5 vertices from data0.txt: [1, 2, 3, 4, 5]\n",
            "Attempting to load: data1.txt\n",
            "Loaded 18 vertices from data1.txt: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18]\n",
            "Attempting to load: data2.txt\n",
            "Loaded 19 vertices from data2.txt: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "Attempting to load: data3.txt\n",
            "Loaded 19 vertices from data3.txt: [1, 2, 3, 4, 5, 6, 7, 8, 9, 10, 11, 12, 13, 14, 15, 16, 17, 18, 19]\n",
            "\n",
            "Dataset: Dataset 0 (5 vertices) (5 vertices)\n",
            "--------------------------------------------------\n",
            "Hill Climbing:\n",
            "Ordering: (5, 3, 4, 1, 2)\n",
            "Total Cost: 465.434\n",
            "\n",
            "Simulated Annealing:\n",
            "Ordering: (2, 3, 5, 4, 1)\n",
            "Total Cost: 466.306\n",
            "\n",
            "BFS:\n",
            "Ordering: (4, 2, 5, 3, 1)\n",
            "Total Cost: 465.434\n",
            "\n",
            "DFS (Brute Force):\n",
            "Ordering: (4, 2, 5, 3, 1)\n",
            "Total Cost: 465.434\n",
            "\n",
            "Minimax (Simplified):\n",
            "Ordering: (4, 2, 5, 3, 1)\n",
            "Total Cost: 465.434\n",
            "\n",
            "Greedy Best-First Search:\n",
            "Ordering: (2, 4, 5, 3, 1)\n",
            "Total Cost: 465.435\n",
            "\n",
            "A* Search:\n",
            "Ordering: (4, 2, 5, 3, 1)\n",
            "Total Cost: 465.434\n",
            "\n",
            "Uniform Cost Search:\n",
            "Ordering: (4, 2, 5, 3, 1)\n",
            "Total Cost: 465.434\n",
            "\n",
            "\n",
            "Dataset: Dataset 1 (18 vertices) (18 vertices)\n",
            "--------------------------------------------------\n",
            "Skipping DFS, BFS, and Minimax due to large size (factorial complexity).\n",
            "Hill Climbing:\n",
            "Ordering: (13, 17, 14, 4, 1, 10, 15, 9, 16, 5, 11, 6, 3, 2, 8, 12, 7, 18)\n",
            "Total Cost: 3205.669\n",
            "\n",
            "Simulated Annealing:\n",
            "Ordering: (8, 5, 16, 6, 12, 13, 17, 11, 10, 2, 18, 3, 15, 9, 1, 7, 4, 14)\n",
            "Total Cost: 3220.052\n",
            "\n",
            "Greedy Best-First Search:\n",
            "Ordering: (7, 13, 12, 16, 14, 15, 8, 9, 17, 4, 10, 6, 11, 18, 3, 5, 2, 1)\n",
            "Total Cost: 3243.777\n",
            "\n",
            "A* Search hit state limit of 1,000,000; result may be suboptimal.\n",
            "A* Search:\n",
            "Ordering: None\n",
            "Total Cost: inf\n",
            "\n",
            "Uniform Cost Search hit state limit of 1,000,000; result may be suboptimal.\n",
            "Uniform Cost Search:\n",
            "Ordering: None\n",
            "Total Cost: inf\n",
            "\n",
            "\n",
            "Dataset: Dataset 2 (19 vertices) (19 vertices)\n",
            "--------------------------------------------------\n",
            "Skipping DFS, BFS, and Minimax due to large size (factorial complexity).\n",
            "Hill Climbing:\n",
            "Ordering: (8, 1, 10, 9, 3, 6, 19, 18, 11, 14, 4, 13, 15, 5, 12, 16, 17, 2, 7)\n",
            "Total Cost: 1974.025\n",
            "\n",
            "Simulated Annealing:\n",
            "Ordering: (11, 7, 14, 16, 8, 3, 17, 19, 13, 2, 12, 9, 18, 4, 6, 5, 15, 10, 1)\n",
            "Total Cost: 2005.156\n",
            "\n",
            "Greedy Best-First Search:\n",
            "Ordering: (5, 10, 8, 4, 6, 18, 3, 17, 9, 7, 19, 1, 12, 16, 14, 15, 13, 2, 11)\n",
            "Total Cost: 1993.182\n",
            "\n",
            "A* Search hit state limit of 1,000,000; result may be suboptimal.\n",
            "A* Search:\n",
            "Ordering: None\n",
            "Total Cost: inf\n",
            "\n",
            "Uniform Cost Search hit state limit of 1,000,000; result may be suboptimal.\n",
            "Uniform Cost Search:\n",
            "Ordering: None\n",
            "Total Cost: inf\n",
            "\n",
            "\n",
            "Dataset: Dataset 3 (19 vertices) (19 vertices)\n",
            "--------------------------------------------------\n",
            "Skipping DFS, BFS, and Minimax due to large size (factorial complexity).\n",
            "Hill Climbing:\n",
            "Ordering: (9, 5, 2, 10, 6, 1, 13, 12, 18, 15, 8, 19, 17, 11, 14, 3, 16, 7, 4)\n",
            "Total Cost: 7992.697\n",
            "\n",
            "Simulated Annealing:\n",
            "Ordering: (18, 12, 14, 17, 8, 1, 11, 2, 13, 6, 10, 7, 19, 15, 4, 16, 5, 9, 3)\n",
            "Total Cost: 7993.936\n",
            "\n",
            "Greedy Best-First Search:\n",
            "Ordering: (5, 6, 14, 4, 11, 12, 9, 8, 3, 7, 2, 10, 13, 1, 15, 17, 18, 16, 19)\n",
            "Total Cost: 8111.977\n",
            "\n",
            "A* Search hit state limit of 1,000,000; result may be suboptimal.\n",
            "A* Search:\n",
            "Ordering: None\n",
            "Total Cost: inf\n",
            "\n",
            "Uniform Cost Search hit state limit of 1,000,000; result may be suboptimal.\n",
            "Uniform Cost Search:\n",
            "Ordering: None\n",
            "Total Cost: inf\n",
            "\n"
          ]
        }
      ],
      "source": [
        "import itertools\n",
        "import random\n",
        "import math\n",
        "from queue import Queue, PriorityQueue\n",
        "from copy import deepcopy\n",
        "\n",
        "# Name: Muhammad Abdullah\n",
        "# Student ID: p22-9371\n",
        "# Assignment#: Assignment no 2\n",
        "# Section: BCS-6C\n",
        "\n",
        "# Function to parse dataset from a file\n",
        "def parse_dataset(file_path):\n",
        "    data = {}\n",
        "    print(f\"Attempting to load: {file_path}\")\n",
        "    with open(file_path, 'r') as file:\n",
        "        for line_num, line in enumerate(file, 1):\n",
        "            line = line.strip()\n",
        "            if not line:\n",
        "                continue\n",
        "            try:\n",
        "                first_comma = line.index(',')\n",
        "                last_comma = line.rindex(',')\n",
        "                vertex = int(line[:first_comma])\n",
        "                parent_set_str = line[first_comma + 1:last_comma]\n",
        "                cost = float(line[last_comma + 1:])\n",
        "                parent_set = set(eval(parent_set_str))\n",
        "                parent_set = {int(p) for p in parent_set}\n",
        "                if vertex not in data:\n",
        "                    data[vertex] = []\n",
        "                data[vertex].append((parent_set, cost))\n",
        "            except Exception as e:\n",
        "                print(f\"Error in {file_path}, line {line_num}: {line} - {e}\")\n",
        "    vertices = sorted(data.keys())\n",
        "    print(f\"Loaded {len(vertices)} vertices from {file_path}: {vertices}\")\n",
        "    return data, vertices\n",
        "\n",
        "# Load datasets\n",
        "dataset_files = {\n",
        "    \"Dataset 0 (5 vertices)\": \"data0.txt\",\n",
        "    \"Dataset 1 (18 vertices)\": \"data1.txt\",\n",
        "    \"Dataset 2 (19 vertices)\": \"data2.txt\",\n",
        "    \"Dataset 3 (19 vertices)\": \"data3.txt\"\n",
        "}\n",
        "\n",
        "expected_counts = {\n",
        "    \"Dataset 0 (5 vertices)\": 5,\n",
        "    \"Dataset 1 (18 vertices)\": 18,\n",
        "    \"Dataset 2 (19 vertices)\": 19,\n",
        "    \"Dataset 3 (19 vertices)\": 19\n",
        "}\n",
        "\n",
        "datasets = {}\n",
        "for name, file_path in dataset_files.items():\n",
        "    try:\n",
        "        data, vertices = parse_dataset(file_path)\n",
        "        if len(vertices) != expected_counts[name]:\n",
        "            print(f\"Warning: {name} has {len(vertices)} vertices, expected {expected_counts[name]}.\")\n",
        "        datasets[name] = (data, vertices)\n",
        "    except FileNotFoundError:\n",
        "        print(f\"Error: File '{file_path}' not found. Skipping dataset '{name}'.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error parsing '{file_path}': {e}. Skipping dataset '{name}'.\")\n",
        "\n",
        "if not datasets:\n",
        "    raise SystemExit(\"No datasets loaded.\")\n",
        "\n",
        "# Cost functions\n",
        "def min_consistent_cost(vertex, ordering, data):\n",
        "    pos = ordering.index(vertex)\n",
        "    available = set(ordering[:pos])\n",
        "    min_cost = float('inf')\n",
        "    for parent_set, cost in data[vertex]:\n",
        "        if parent_set.issubset(available):\n",
        "            min_cost = min(min_cost, cost)\n",
        "    return min_cost\n",
        "\n",
        "def total_cost(ordering, data):\n",
        "    return sum(min_consistent_cost(vertex, ordering, data) for vertex in ordering)\n",
        "\n",
        "# 1. Hill Climbing\n",
        "def hill_climbing(vertices, data):\n",
        "    current = list(vertices)\n",
        "    random.shuffle(current)\n",
        "    current_cost = total_cost(current, data)\n",
        "    while True:\n",
        "        neighbors = []\n",
        "        for i in range(len(current)):\n",
        "            for j in range(i + 1, len(current)):\n",
        "                neighbor = current.copy()\n",
        "                neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n",
        "                neighbors.append(neighbor)\n",
        "        best_neighbor = min(neighbors, key=lambda x: total_cost(x, data), default=current)\n",
        "        neighbor_cost = total_cost(best_neighbor, data)\n",
        "        if neighbor_cost >= current_cost:\n",
        "            break\n",
        "        current = best_neighbor\n",
        "        current_cost = neighbor_cost\n",
        "    return tuple(current), current_cost\n",
        "\n",
        "# 2. Simulated Annealing\n",
        "def simulated_annealing(vertices, data, initial_temp=1000, cooling_rate=0.95):\n",
        "    current = list(vertices)\n",
        "    random.shuffle(current)\n",
        "    current_cost = total_cost(current, data)\n",
        "    temp = initial_temp\n",
        "    while temp > 1:\n",
        "        i, j = random.sample(range(len(current)), 2)\n",
        "        neighbor = current.copy()\n",
        "        neighbor[i], neighbor[j] = neighbor[j], neighbor[i]\n",
        "        neighbor_cost = total_cost(neighbor, data)\n",
        "        if neighbor_cost < current_cost or random.random() < math.exp((current_cost - neighbor_cost) / temp):\n",
        "            current = neighbor\n",
        "            current_cost = neighbor_cost\n",
        "        temp *= cooling_rate\n",
        "    return tuple(current), current_cost\n",
        "\n",
        "# 3. BFS\n",
        "def bfs_search(vertices, data):\n",
        "    queue = Queue()\n",
        "    queue.put([])\n",
        "    best_ordering = None\n",
        "    best_cost = float('inf')\n",
        "    all_vertices = set(vertices)\n",
        "    while not queue.empty():\n",
        "        current = queue.get()\n",
        "        if len(current) == len(vertices):\n",
        "            cost = total_cost(current, data)\n",
        "            if cost < best_cost:\n",
        "                best_cost = cost\n",
        "                best_ordering = tuple(current)\n",
        "            continue\n",
        "        remaining = all_vertices - set(current)\n",
        "        for v in remaining:\n",
        "            queue.put(current + [v])\n",
        "    return best_ordering, best_cost\n",
        "\n",
        "# 4. DFS (Brute Force)\n",
        "def dfs_search(vertices, data):\n",
        "    best_ordering = None\n",
        "    best_cost = float('inf')\n",
        "    for perm in itertools.permutations(vertices):\n",
        "        cost = total_cost(perm, data)\n",
        "        if cost < best_cost:\n",
        "            best_cost = cost\n",
        "            best_ordering = perm\n",
        "    return best_ordering, best_cost\n",
        "\n",
        "# 5. Minimax (Simplified)\n",
        "def minimax_search(vertices, data):\n",
        "    best_ordering = None\n",
        "    best_cost = float('inf')\n",
        "    for perm in itertools.permutations(vertices):\n",
        "        cost = total_cost(perm, data)\n",
        "        if cost < best_cost:\n",
        "            best_cost = cost\n",
        "            best_ordering = perm\n",
        "    return best_ordering, best_cost\n",
        "\n",
        "# 6. Greedy Best-First Search\n",
        "def greedy_best_first_search(vertices, data):\n",
        "    all_vertices = set(vertices)\n",
        "    ordering = []\n",
        "    while len(ordering) < len(vertices):\n",
        "        remaining = all_vertices - set(ordering)\n",
        "        best_vertex = min(remaining, key=lambda v: min_consistent_cost(v, ordering + [v], data))\n",
        "        ordering.append(best_vertex)\n",
        "    cost = total_cost(ordering, data)\n",
        "    return tuple(ordering), cost\n",
        "\n",
        "# 7. Optimized A* Search - Constraint-aware heuristic, lower cap\n",
        "def a_star_search(vertices, data):\n",
        "    def heuristic(partial, all_vertices, data):\n",
        "        remaining = set(all_vertices) - set(partial)\n",
        "        h_cost = 0\n",
        "        placed = set(partial)\n",
        "        for v in remaining:\n",
        "            min_cost = float('inf')\n",
        "            for parent_set, cost in data[v]:\n",
        "                # Only count if all parents are either placed or still available\n",
        "                if parent_set.issubset(placed) or parent_set.issubset(placed.union(remaining)):\n",
        "                    min_cost = min(min_cost, cost)\n",
        "            h_cost += min_cost if min_cost != float('inf') else 0  # Default to 0 if no valid option yet\n",
        "        return h_cost\n",
        "\n",
        "    pq = PriorityQueue()\n",
        "    pq.put((0, [], 0))  # f_score, ordering, g_score\n",
        "    all_vertices = set(vertices)\n",
        "    state_count = 0\n",
        "    max_states = 1000000  # Reduced cap, e.g., 1 million\n",
        "    best_ordering = None\n",
        "    best_cost = float('inf')\n",
        "\n",
        "    while not pq.empty() and state_count < max_states:\n",
        "        f_score, current, g_score = pq.get()\n",
        "        state_count += 1\n",
        "\n",
        "        if len(current) == len(vertices):\n",
        "            if g_score < best_cost:\n",
        "                best_cost = g_score\n",
        "                best_ordering = tuple(current)\n",
        "            continue\n",
        "\n",
        "        remaining = all_vertices - set(current)\n",
        "        for v in remaining:\n",
        "            new_order = current + [v]\n",
        "            g = total_cost(new_order, data)\n",
        "            if g >= best_cost:  # Prune if already worse than best complete solution\n",
        "                continue\n",
        "            h = heuristic(new_order, all_vertices, data)\n",
        "            f = g + h\n",
        "            if f < best_cost:  # Only explore promising paths\n",
        "                pq.put((f, new_order, g))\n",
        "\n",
        "    if state_count >= max_states:\n",
        "        print(\"A* Search hit state limit of 1,000,000; result may be suboptimal.\")\n",
        "    return best_ordering, best_cost if best_ordering else float('inf')\n",
        "\n",
        "# 8. Uniform Cost Search - Kept for comparison\n",
        "def uniform_cost_search(vertices, data):\n",
        "    pq = PriorityQueue()\n",
        "    pq.put((0, []))\n",
        "    best_ordering = None\n",
        "    best_cost = float('inf')\n",
        "    all_vertices = set(vertices)\n",
        "    state_count = 0\n",
        "    max_states = 1000000  # Matching A* for fairness\n",
        "    while not pq.empty() and state_count < max_states:\n",
        "        cost_so_far, current = pq.get()\n",
        "        state_count += 1\n",
        "        if len(current) == len(vertices):\n",
        "            if cost_so_far < best_cost:\n",
        "                best_cost = cost_so_far\n",
        "                best_ordering = tuple(current)\n",
        "            continue\n",
        "        remaining = all_vertices - set(current)\n",
        "        for v in remaining:\n",
        "            new_order = current + [v]\n",
        "            new_cost = total_cost(new_order, data)\n",
        "            pq.put((new_cost, new_order))\n",
        "    if state_count >= max_states:\n",
        "        print(\"Uniform Cost Search hit state limit of 1,000,000; result may be suboptimal.\")\n",
        "    return best_ordering, best_cost if best_ordering else float('inf')\n",
        "\n",
        "# Main execution\n",
        "searches = [\n",
        "    (\"Hill Climbing\", hill_climbing),\n",
        "    (\"Simulated Annealing\", simulated_annealing),\n",
        "    (\"BFS\", bfs_search),\n",
        "    (\"DFS (Brute Force)\", dfs_search),\n",
        "    (\"Minimax (Simplified)\", minimax_search),\n",
        "    (\"Greedy Best-First Search\", greedy_best_first_search),\n",
        "    (\"A* Search\", a_star_search),\n",
        "    (\"Uniform Cost Search\", uniform_cost_search)\n",
        "]\n",
        "\n",
        "for name, (data, vertices) in datasets.items():\n",
        "    print(f\"\\nDataset: {name} ({len(vertices)} vertices)\")\n",
        "    print(\"-\" * 50)\n",
        "    if len(vertices) > 5:\n",
        "        limited_searches = [(n, f) for n, f in searches if n not in [\"DFS (Brute Force)\", \"BFS\", \"Minimax (Simplified)\"]]\n",
        "        print(\"Skipping DFS, BFS, and Minimax due to large size (factorial complexity).\")\n",
        "    else:\n",
        "        limited_searches = searches\n",
        "\n",
        "    for search_name, search_func in limited_searches:\n",
        "        try:\n",
        "            ordering, cost = search_func(vertices, data)\n",
        "            print(f\"{search_name}:\")\n",
        "            print(f\"Ordering: {ordering}\")\n",
        "            print(f\"Total Cost: {cost:.3f}\\n\")\n",
        "        except Exception as e:\n",
        "            print(f\"Error in {search_name} for {name}: {e}\\n\")"
      ]
    }
  ]
}